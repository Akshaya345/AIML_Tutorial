{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMiiyKyhjzAmDyUgmvxC85n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akshaya345/AIML_Tutorial/blob/main/AIML_Module1_Lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "1QtJTvwt36zx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "rand=np.random.default_rng(seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=datasets.fetch_california_housing()\n",
        "print(dataset.DESCR)\n",
        "print(dataset.keys())\n",
        "dataset.target=dataset.target.astype(np.int)\n",
        "print()\n",
        "print()\n",
        "print(dataset.data.shape)\n",
        "print(dataset.target.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwU5uRnA5obh",
        "outputId": "3eec8c00-5123-4e62-f8d8-f1581f0fa174"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. _california_housing_dataset:\n",
            "\n",
            "California Housing dataset\n",
            "--------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 20640\n",
            "\n",
            "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
            "\n",
            "    :Attribute Information:\n",
            "        - MedInc        median income in block group\n",
            "        - HouseAge      median house age in block group\n",
            "        - AveRooms      average number of rooms per household\n",
            "        - AveBedrms     average number of bedrooms per household\n",
            "        - Population    block group population\n",
            "        - AveOccup      average number of household members\n",
            "        - Latitude      block group latitude\n",
            "        - Longitude     block group longitude\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "\n",
            "This dataset was obtained from the StatLib repository.\n",
            "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
            "\n",
            "The target variable is the median house value for California districts,\n",
            "expressed in hundreds of thousands of dollars ($100,000).\n",
            "\n",
            "This dataset was derived from the 1990 U.S. census, using one row per census\n",
            "block group. A block group is the smallest geographical unit for which the U.S.\n",
            "Census Bureau publishes sample data (a block group typically has a population\n",
            "of 600 to 3,000 people).\n",
            "\n",
            "A household is a group of people residing within a home. Since the average\n",
            "number of rooms and bedrooms in this dataset are provided per household, these\n",
            "columns may take surprisingly large values for block groups with few households\n",
            "and many empty houses, such as vacation resorts.\n",
            "\n",
            "It can be downloaded/loaded using the\n",
            ":func:`sklearn.datasets.fetch_california_housing` function.\n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
            "      Statistics and Probability Letters, 33 (1997) 291-297\n",
            "\n",
            "dict_keys(['data', 'target', 'frame', 'target_names', 'feature_names', 'DESCR'])\n",
            "\n",
            "\n",
            "(20640, 8)\n",
            "(20640,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-1fcdf5f9cf1d>:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  dataset.target=dataset.target.astype(np.int)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def NN1(traindata,trainlabel,query):\n",
        "  diff=traindata-query\n",
        "  square=diff*diff\n",
        "  dist=square.sum(1)\n",
        "  label=trainlabel[np.argmin(dist)]\n",
        "  return label\n",
        "def NN(traindata,trainlabel,testdata):\n",
        "  predlabel=np.array([NN1(traindata,trainlabel,i) for i in testdata])\n",
        "  return predlabel"
      ],
      "metadata": {
        "id": "kLaobwME51-g"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RandomClassifier(traindata,trainlabel,testdata):\n",
        "  classes=np.unique(trainlabel)\n",
        "  rints=rand.integers(low=0,high=len(classes),size=len(testdata))\n",
        "  predlabel=classes[rints]\n",
        "  return predlabel"
      ],
      "metadata": {
        "id": "g_OojpNC8a8I"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Accuracy(gtlabel, predlabel):\n",
        "  assert len(gtlabel)==len(predlabel), \"Length of the groundtruth labels and predicted labels should be the same\"\n",
        "  correct=(gtlabel==predlabel).sum()\n",
        "  return correct/len(gtlabel)"
      ],
      "metadata": {
        "id": "45mSb6zT80LY"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split(data, label, percent):\n",
        "  rnd=rand.random(len(label))\n",
        "  split1=rnd<percent\n",
        "  split2=rnd>=percent\n",
        "  split1data=data[split1,:]\n",
        "  split1label=label[split1]\n",
        "  split2data=data[split2,:]\n",
        "  split2label=label[split2]\n",
        "  return split1data, split1label, split2data, split2label"
      ],
      "metadata": {
        "id": "ku3ImOt584Qb"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testdata,testlabel,alltraindata,alltrainlabel=split(dataset.data,dataset.target,20/100)\n",
        "print('Number of test samples=', len(testlabel))\n",
        "print('Number of other samples=', len(alltrainlabel))\n",
        "print('Percent of test data=', len(testlabel)*100/len(dataset.target),'%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GF9IWFG09nIF",
        "outputId": "79fbdc6a-f9bb-4286-a479-b85e3a87e43f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test samples= 4144\n",
            "Number of other samples= 16496\n",
            "Percent of test data= 20.07751937984496 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "traindata,trainlabel,valdata,vallabel=split(alltraindata,alltrainlabel,75/100)"
      ],
      "metadata": {
        "id": "k4EuezQg94jZ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainprediction=NN(traindata,trainlabel,traindata)\n",
        "trainAccuracy=Accuracy(trainlabel,trainprediction)\n",
        "print(\"Train accuracy using nearest neighbour is: \", trainAccuracy)\n",
        "trainprediction=RandomClassifier(traindata,trainlabel,traindata)\n",
        "trainAccuracy=Accuracy(trainlabel,trainprediction)\n",
        "print(\"Train accuracy using random classifier is: \", trainAccuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6Uyi-3Y-D4J",
        "outputId": "65ba2030-edda-4c0b-91a9-5af04963a2bb"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy using nearest neighbour is:  1.0\n",
            "Train accuracy using random classifier is:  0.164375808538163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valprediction=NN(traindata,trainlabel,valdata)\n",
        "valAccuracy=Accuracy(vallabel,valprediction)\n",
        "print(\"Validation accuracy using nearest neighbour is: \",valAccuracy)\n",
        "valprediction=RandomClassifier(traindata,trainlabel,valdata)\n",
        "valAccuracy=Accuracy(vallabel,valprediction)\n",
        "print(\"Validation accuracy using random classifier is: \",valAccuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRo_c_AIdGwy",
        "outputId": "de0c88eb-1905-404d-bb26-d85164fde4cc"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour is:  0.34108527131782945\n",
            "Validation accuracy using random classifier is:  0.1688468992248062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "traindata,trainlabel,valdata,vallabel=split(alltraindata,alltrainlabel,75/100)\n",
        "valpred=NN(traindata,trainlabel,valdata)\n",
        "valAccuracy=Accuracy(vallabel,valpred)\n",
        "print(\"Validation accuracy of nearest neighbour is: \", valAccuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsCyBkMG-kMj",
        "outputId": "75944e9c-c271-42ac-9658-1834c5be9d0c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy of nearest neighbour is:  0.34048257372654156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testprediction=NN(alltraindata,alltrainlabel,testdata)\n",
        "testAccuracy=Accuracy(testlabel,testprediction)\n",
        "print('Test accuracy is: ', testAccuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSx9r7Dg_7sr",
        "outputId": "4b391210-8905-4f43-c7f3-657513acc494"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy is:  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. How is the accuracy of the validation set affected if we increase the percentage of validation set? What happens when we reduce it?**\n",
        "\n",
        "**Ans:** When the percentage of the validation set is increased,we receive a more reliable estimate of the model's performance,which often leads to improved validation accuracy in **nearest neighbour** whereas increasing the percentage of the validation set does not significantly impact accuracy for **random classifier**. Also,we have fewer data for training,which may result in underfitting.\n",
        "\n",
        "**2. How does the size of the train and validation set affect how well we can predict the accuracy on the test set using the validation set?**\n",
        "\n",
        "**Ans:** The size of the train set affects the model's ability to learn, with larger sets potentially improving predictive accuracy. A larger validation set provides a more reliable estimate of model performance but should be balanced with an adequately sized training set for accurate predictions on the test set.\n",
        "\n",
        "**3. What do you think is a good percentage to reserve for the validation set so that these two factors are balanced?**\n",
        "\n",
        "**Ans:** For both nearest neighbor and random classifier, a good percentage to reserve for the validation set to achieve a balance typically falls in the range of 10% to 30%."
      ],
      "metadata": {
        "id": "np1LBfS5dPfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "traindata,trainlabel,valdata,vallabel=split(alltraindata,alltrainlabel,40/100)\n",
        "valprediction=NN(traindata,trainlabel,valdata)\n",
        "valAccuracy=Accuracy(vallabel,valprediction)\n",
        "print(\"Validation accuracy using nearest neighbour is: \",valAccuracy)\n",
        "valprediction=RandomClassifier(traindata,trainlabel,valdata)\n",
        "valAccuracy=Accuracy(vallabel,valprediction)\n",
        "print(\"Validation accuracy using random classifier is: \",valAccuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygt2PP13eT0i",
        "outputId": "07291af6-58ea-45f0-c41c-ff0a7aeaaf17"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour is:  0.3348187158193235\n",
            "Validation accuracy using random classifier is:  0.16832084261697386\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testprediction=NN(alltraindata,alltrainlabel,testdata)\n",
        "testAccuracy=Accuracy(testlabel,testprediction)\n",
        "print('Test accuracy is: ', testAccuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9h8wPdKeq7J",
        "outputId": "62024f73-4028-4389-f6ea-f273b14200df"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy is:  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def AverageAccuracy(alldata,alllabel,splitpercent,iterations,classifier=NN):\n",
        "  accuracy = 0\n",
        "  for i in range(iterations):\n",
        "    traindata,trainlabel,valdata,vallabel=split(alldata,alllabel,splitpercent)\n",
        "    valprediction=classifier(traindata,trainlabel,valdata)\n",
        "    accuracy+=Accuracy(vallabel,valprediction)\n",
        "  return accuracy/iterations"
      ],
      "metadata": {
        "id": "xMcSjPPDiYXH"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Average validation accuracy is: ',AverageAccuracy(alltraindata,alltrainlabel,75/100,10,classifier=NN))\n",
        "testprediction=NN(alltraindata,alltrainlabel,testdata)\n",
        "print('Test accuracy is: ',Accuracy(testlabel,testprediction) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cW4SLzLc-GvN",
        "outputId": "86ee1496-a27e-4e6f-9026-43e90b870ffd"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average validation accuracy is:  0.3359366875267045\n",
            "Test accuracy is:  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.** **Does averaging the validation accuracy across multiple splits give more consistent results?**\n",
        "\n",
        "**Ans:** Yes,cross-validation, or averaging validation accuracy across multiple splits, provides a more stable and less biassed estimate of a model's performance by reducing the impact of a single data split's randomness and ensuring a more comprehensive assessment of its generalisation capability. This method helps in obtaining a more accurate depiction of a model's expected performance on previously unknown data.\n",
        "\n",
        "\n",
        "**2.** **Does it give more accurate estimate of test accuracy?**\n",
        "\n",
        "**Ans:** Yes, it provides a more accurate estimate of a model's likely performance on unseen data. It reduces the bias and variability associated with a single data split, resulting in a more reliable prediction of how the model will perform on new, unseen data.\n",
        "\n",
        "**3.** **What is the effect of the number of iterations on the estimate? Do we get a better estimate with higher iterations?**\n",
        "\n",
        "**Ans:** Increasing the number of iterations during training can lead to a more accurate estimate of test accuracy up to a certain point. As the model iterates and refines its parameters, it becomes better at capturing patterns in the training data, potentially improving its overall performance.\n",
        "\n",
        "However, it's essential to keep track of the model's effectiveness using a different validation set or cross-validation. This reduces overfitting and helps determine when the model has reached its peak performance. Overfitting happens when the model becomes overly dependent on the training set, which reduces the model's capacity to generalise. The model's prediction of test accuracy may become less accurate in such circumstances.\n",
        "\n",
        "Therefore, while increasing the number of iterations can improve the model's performance and its test accuracy estimate, careful monitoring for overfitting is required to ensure the accuracy of the estimate for unknown data.\n",
        "\n",
        "**4. Consider the results you got for the previous questions. Can we deal with a very small train dataset or validation dataset by increasing the iterations?**\n",
        "\n",
        "**Ans:** Although more iterations can improve model performance, they cannot entirely make up for a very short training or validation dataset. Small datasets can lead to overfitting. A careful balance between dataset size, model complexity, and training iterations is necessary to get accurate findings.\n"
      ],
      "metadata": {
        "id": "mW3Iw6kK_AqE"
      }
    }
  ]
}